{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8962e6b0",
      "metadata": {
        "id": "8962e6b0"
      },
      "source": [
        "# CNN Architecture Assignment\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ced48fe",
      "metadata": {
        "id": "6ced48fe"
      },
      "source": [
        "## Question 1\n",
        "**What is the role of filters and feature maps in Convolutional Neural Network (CNN)?**\n",
        "\n",
        "**Answer:**\n",
        "Filters (kernels) are small matrices that slide over the input image to detect specific patterns such as edges, textures, or shapes. Each filter performs a convolution operation to extract a particular feature.\n",
        "\n",
        "Feature maps are the outputs produced after applying filters to the input. Each feature map highlights the presence and location of a specific feature detected by its corresponding filter. Together, filters and feature maps enable CNNs to learn hierarchical visual features."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc3050e7",
      "metadata": {
        "id": "fc3050e7"
      },
      "source": [
        "## Question 2\n",
        "**Explain the concepts of padding and stride in CNNs. How do they affect output dimensions?**\n",
        "\n",
        "**Answer:**\n",
        "Padding involves adding extra pixels (usually zeros) around the input image to control the spatial size of the output. Stride defines the step size by which the filter moves across the input.\n",
        "\n",
        "Padding helps preserve spatial dimensions, while larger strides reduce output size. Output size depends on input size, filter size, padding, and stride."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97a2342b",
      "metadata": {
        "id": "97a2342b"
      },
      "source": [
        "## Question 3\n",
        "**Define receptive field in CNNs. Why is it important?**\n",
        "\n",
        "**Answer:**\n",
        "The receptive field is the region of the input image that affects a particular neuron in a CNN. As depth increases, the receptive field grows, allowing neurons to capture larger contextual information.\n",
        "\n",
        "It is important because larger receptive fields help deep CNNs understand complex patterns and global structures."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f47c776",
      "metadata": {
        "id": "6f47c776"
      },
      "source": [
        "## Question 4\n",
        "**Discuss how filter size and stride influence the number of parameters in a CNN.**\n",
        "\n",
        "**Answer:**\n",
        "Filter size directly affects the number of parameters: larger filters have more weights. Stride does not change the number of parameters but affects the output feature map size. Smaller filters with deeper architectures are preferred for efficiency."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f97b9880",
      "metadata": {
        "id": "f97b9880"
      },
      "source": [
        "## Question 5\n",
        "**Compare and contrast LeNet, AlexNet, and VGG.**\n",
        "\n",
        "**Answer:**\n",
        "- **LeNet:** Shallow network, small filters, designed for digit recognition.\n",
        "- **AlexNet:** Deeper network, larger filters, introduced ReLU and dropout.\n",
        "- **VGG:** Very deep network with small (3×3) filters, high accuracy but computationally expensive."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6:\n",
        "**Using keras, build and train a simple CNN model on the MNIST dataset from scratch. Include code for module creation, compilation, training, and evaluation.**"
      ],
      "metadata": {
        "id": "BpMV2AQFfDr-"
      },
      "id": "BpMV2AQFfDr-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "318a2f61",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "318a2f61",
        "outputId": "96c7e73c-2ddd-44d0-c29c-cc68c838d53a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.9096 - loss: 0.3033\n",
            "Epoch 2/3\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9835 - loss: 0.0550\n",
            "Epoch 3/3\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9901 - loss: 0.0325\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9826 - loss: 0.0569\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.046846337616443634, 0.9848999977111816]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train[..., None] / 255.0\n",
        "x_test = x_test[..., None] / 255.0\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=3)\n",
        "model.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 7:\n",
        "**Load and preprocess the CIFAR-10 dataset using Keras, and create a CNN model to classify RGB images. Show your preprocessing and architecture.**"
      ],
      "metadata": {
        "id": "G0ynSkMJfQnq"
      },
      "id": "G0ynSkMJfQnq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1edda8a5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1edda8a5",
        "outputId": "ff1392aa-a3b2-4775-adf1-2e55f6977bf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.3915 - loss: 1.6593\n",
            "Epoch 2/5\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.6160 - loss: 1.0958\n",
            "Epoch 3/5\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.6697 - loss: 0.9448\n",
            "Epoch 4/5\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7052 - loss: 0.8439\n",
            "Epoch 5/5\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7295 - loss: 0.7715\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6896 - loss: 0.9079\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9062128067016602, 0.6906999945640564]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train, x_test = x_train/255.0, x_test/255.0\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "model.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 8:\n",
        " **Using PyTorch, write a script to define and train a CNN on the MNIST dataset. Include model definition, data loaders, training loop, and accuracy evaluation.**"
      ],
      "metadata": {
        "id": "Cgn6q5uwfYDa"
      },
      "id": "Cgn6q5uwfYDa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f955e56",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f955e56",
        "outputId": "9797d221-5d03-4d6b-aab9-18a70566c492"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5] - Loss: 112.4094\n",
            "Epoch [2/5] - Loss: 35.6354\n",
            "Epoch [3/5] - Loss: 21.6731\n",
            "Epoch [4/5] - Loss: 14.7206\n",
            "Epoch [5/5] - Loss: 10.6947\n",
            "Test Accuracy: 98.98%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 1. Device configuration (CPU / GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 2. Data preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# 3. Load MNIST dataset\n",
        "train_dataset = datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    transform=transform,\n",
        "    download=True\n",
        ")\n",
        "\n",
        "test_dataset = datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    transform=transform,\n",
        "    download=True\n",
        ")\n",
        "\n",
        "# 4. Data loaders\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# 5. CNN model definition\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # ✅ Corrected feature size: 64 × 12 × 12 = 9216\n",
        "        self.fc1 = nn.Linear(64 * 12 * 12, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)   # Flatten\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# 6. Initialize model\n",
        "model = CNN().to(device)\n",
        "\n",
        "# 7. Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 8. Training loop\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {running_loss:.4f}\")\n",
        "\n",
        "# 9. Model evaluation (Accuracy)\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 9:\n",
        "**Given a custom image dataset stored in a local directory, write code using Keras ImageDataGenerator to preprocess and train a CNN model.**"
      ],
      "metadata": {
        "id": "S_sp_FBBfhPa"
      },
      "id": "S_sp_FBBfhPa"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1ac3b3fd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ac3b3fd",
        "outputId": "5669426d-0b0e-4c49-cc2b-e4ed0a2c53b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10 images belonging to 2 classes.\n",
            "Found 10 images belonging to 2 classes.\n",
            "Epoch 1/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.6000 - loss: 0.6976 - val_accuracy: 0.5000 - val_loss: 0.9761\n",
            "Epoch 2/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.5000 - loss: 1.1078 - val_accuracy: 0.5000 - val_loss: 2.9861\n",
            "Epoch 3/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.5000 - loss: 2.3480 - val_accuracy: 0.5000 - val_loss: 0.7130\n",
            "Epoch 4/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.7000 - loss: 0.6081 - val_accuracy: 0.5000 - val_loss: 1.3404\n",
            "Epoch 5/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.5000 - loss: 1.1835 - val_accuracy: 0.5000 - val_loss: 0.9614\n",
            "Epoch 6/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.5000 - loss: 0.9656 - val_accuracy: 0.5000 - val_loss: 0.7059\n",
            "Epoch 7/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.5000 - loss: 0.7014 - val_accuracy: 0.5000 - val_loss: 0.7052\n",
            "Epoch 8/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.5000 - loss: 0.7083 - val_accuracy: 0.5000 - val_loss: 0.7063\n",
            "Epoch 9/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.5000 - loss: 0.6945 - val_accuracy: 0.5000 - val_loss: 0.7005\n",
            "Epoch 10/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.5000 - loss: 0.6845 - val_accuracy: 0.5000 - val_loss: 0.6980\n",
            "Epoch 11/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.5000 - loss: 0.6990 - val_accuracy: 0.5000 - val_loss: 0.6944\n",
            "Epoch 12/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.6000 - loss: 0.6876 - val_accuracy: 0.5000 - val_loss: 0.6947\n",
            "Epoch 13/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.4000 - loss: 0.7011 - val_accuracy: 0.5000 - val_loss: 0.6947\n",
            "Epoch 14/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.6000 - loss: 0.6894 - val_accuracy: 0.5000 - val_loss: 0.6946\n",
            "Epoch 15/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.3000 - loss: 0.7091 - val_accuracy: 0.5000 - val_loss: 0.6962\n",
            "Epoch 16/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.2000 - loss: 0.7313 - val_accuracy: 0.5000 - val_loss: 0.6946\n",
            "Epoch 17/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.5000 - loss: 0.6919 - val_accuracy: 0.5000 - val_loss: 0.6939\n",
            "Epoch 18/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.4000 - loss: 0.7268 - val_accuracy: 0.5000 - val_loss: 0.6930\n",
            "Epoch 19/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.4000 - loss: 0.7147 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
            "Epoch 20/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.5000 - loss: 0.6989 - val_accuracy: 0.5000 - val_loss: 0.6931\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model saved as my_cnn_model.h5\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# 1. SETUP DATA AUGMENTATION\n",
        "# This prevents overfitting by creating variations of your training images\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Validation data should only be rescaled (no augmentation)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# 2. FLOW FROM DIRECTORIES\n",
        "# Update 'class_mode' to 'categorical' if you have more than 2 classes\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/content/Train',\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    '/content/Val',\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# 3. DEFINE THE CNN MODEL\n",
        "model = Sequential([\n",
        "    # First Convolutional Block\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    # Second Convolutional Block\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    # Third Convolutional Block\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    # Flatten and Dense Layers\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5), # Helps prevent overfitting\n",
        "    Dense(1, activation='sigmoid') # Use 'softmax' and units=N for multi-class\n",
        "])\n",
        "\n",
        "# 4. COMPILE\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 5. TRAIN\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    validation_data=val_generator,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 6. SAVE THE MODEL\n",
        "model.save('\\nmy_cnn_model.h5')\n",
        "print(\"\\nModel saved as my_cnn_model.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beef6809",
      "metadata": {
        "id": "beef6809"
      },
      "source": [
        "## Question 10\n",
        "**You are working on a web application for a medical imaging startup. Your task is to build and deploy a CNN model that classifies chest X-ray images into “Normal” and “Pneumonia” categories. Describe your end-to-end approach–from data preparation and model training to deploying the model as a web app using Streamlit. **\n",
        "\n",
        "**Answer:**\n",
        "1. **Data Preparation:** Collect and clean chest X-ray images, apply augmentation and normalization.\n",
        "2. **Model Training:** Use a CNN with transfer learning (e.g., ResNet), binary crossentropy loss, and Adam optimizer.\n",
        "3. **Evaluation:** Use accuracy, recall, and ROC-AUC.\n",
        "4. **Deployment:** Save the trained model and deploy it using Streamlit for real-time predictions via a web interface."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}