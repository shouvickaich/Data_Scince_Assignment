{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "21b0083d",
      "metadata": {
        "id": "21b0083d"
      },
      "source": [
        "# Decision Tree Assignment\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4aacf639",
      "metadata": {
        "id": "4aacf639"
      },
      "source": [
        "## Question 1: What is a Decision Tree, and how does it work in the context of classification?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c165b11",
      "metadata": {
        "id": "9c165b11"
      },
      "source": [
        "**Answer:**  \n",
        "A **Decision Tree** is a supervised machine learning algorithm used for **classification and regression** tasks. It represents decisions and their possible consequences as a **tree-like structure** of nodes and leaves.\n",
        "\n",
        "**How it works (for classification):**  \n",
        "1. The tree starts at a **root node** that contains all data.  \n",
        "2. It **splits** data based on a feature and threshold that best separates classes (using metrics like Gini or Entropy).  \n",
        "3. The process continues recursively, creating **child nodes**.  \n",
        "4. The tree stops growing when a **stopping criterion** is met (e.g., max depth, no further information gain).  \n",
        "5. Each **leaf node** assigns a class label.\n",
        "\n",
        "**Advantages:**  \n",
        "- Easy to understand and visualize  \n",
        "- Handles both categorical and numerical data  \n",
        "- Captures non-linear relationships  \n",
        "\n",
        "**Disadvantages:**  \n",
        "- Prone to overfitting if not pruned  \n",
        "- Can be unstable with small changes in data  \n",
        "- May prefer features with more levels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aac5680f",
      "metadata": {
        "id": "aac5680f"
      },
      "source": [
        "## Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64afe6e3",
      "metadata": {
        "id": "64afe6e3"
      },
      "source": [
        "**Answer:**  \n",
        "Both **Gini Impurity** and **Entropy** measure how mixed the classes are in a node. The goal is to make nodes **as pure as possible**.\n",
        "\n",
        "- **Gini Impurity:**  \n",
        "  Formula:  \n",
        "  \\( Gini = 1 - \\sum p_k^2 \\)  \n",
        "  where \\( p_k \\) is the proportion of class *k* in the node.  \n",
        "  - 0 means completely pure (only one class).  \n",
        "  - Higher Gini means more impurity.\n",
        "\n",
        "- **Entropy:**  \n",
        "  Formula:  \n",
        "  \\( Entropy = -\\sum p_k \\log_2(p_k) \\)  \n",
        "  - 0 means pure.  \n",
        "  - Higher entropy → more disorder.\n",
        "\n",
        "**Impact on splits:**  \n",
        "- Both metrics prefer splits that create purer child nodes.  \n",
        "- Entropy tends to be more sensitive to rare classes; Gini is slightly faster to compute.  \n",
        "- In practice, they produce similar trees."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a226a0ba",
      "metadata": {
        "id": "a226a0ba"
      },
      "source": [
        "## Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f83012e",
      "metadata": {
        "id": "9f83012e"
      },
      "source": [
        "**Answer:**  \n",
        "\n",
        "| Type | Description | Example | Advantage |\n",
        "|------|--------------|----------|------------|\n",
        "| **Pre-Pruning** | Stop tree growth early using parameters like `max_depth`, `min_samples_split`, etc. | Setting `max_depth=5` | Prevents overfitting and reduces computation time |\n",
        "| **Post-Pruning** | Grow a full tree first, then remove weak branches based on validation data or cost complexity (`ccp_alpha`). | Using `DecisionTreeClassifier(ccp_alpha=0.01)` | Usually better generalization since it’s based on model performance |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa12039e",
      "metadata": {
        "id": "aa12039e"
      },
      "source": [
        "## Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57a614d4",
      "metadata": {
        "id": "57a614d4"
      },
      "source": [
        "**Answer:**  \n",
        "**Information Gain (IG)** measures the **reduction in entropy** after a dataset is split based on a feature.  \n",
        "It tells us how much a split improves class purity.\n",
        "\n",
        "Formula:  \n",
        "\\( IG = Entropy(Parent) - \\sum (\\frac{n_i}{n}) Entropy(Child_i) \\)\n",
        "\n",
        "**Importance:**  \n",
        "- High IG means the split produces more homogeneous subsets.  \n",
        "- Decision Trees select the feature with **maximum Information Gain** at each step.  \n",
        "- It helps in building a model that efficiently separates the classes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51d040b3",
      "metadata": {
        "id": "51d040b3"
      },
      "source": [
        "## Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b08df42d",
      "metadata": {
        "id": "b08df42d"
      },
      "source": [
        "**Answer:**  \n",
        "\n",
        "**Applications:**  \n",
        "- Healthcare: Disease diagnosis based on symptoms  \n",
        "- Finance: Credit risk and loan approval  \n",
        "- Marketing: Customer segmentation and churn prediction  \n",
        "- Manufacturing: Fault detection  \n",
        "- Education: Predicting student performance  \n",
        "\n",
        "**Advantages:**  \n",
        "- Easy to interpret and visualize  \n",
        "- No need for feature scaling  \n",
        "- Handles numerical and categorical data  \n",
        "\n",
        "**Limitations:**  \n",
        "- Easily overfits (needs pruning or ensemble methods)  \n",
        "- Sensitive to small data changes  \n",
        "- Greedy algorithm may miss global optimum"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec954cb5",
      "metadata": {
        "id": "ec954cb5"
      },
      "source": [
        "## Question 6: Write a Python program to:\n",
        "- Load the Iris Dataset\n",
        "- Train a Decision Tree Classifier using the Gini criterion\n",
        "- Print the model’s accuracy and feature importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "14bf284e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14bf284e",
        "outputId": "be7bf8a2-ab60-497e-be2f-aa537f8e33ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9333333333333333\n",
            "sepal length (cm): 0.0062\n",
            "sepal width (cm): 0.0292\n",
            "petal length (cm): 0.5586\n",
            "petal width (cm): 0.4060\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Train Decision Tree using Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and accuracy\n",
        "y_pred = clf.predict(X_test)\n",
        "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Feature importance\n",
        "for name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f'{name}: {importance:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cec482e3",
      "metadata": {
        "id": "cec482e3"
      },
      "source": [
        "## Question 7: Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5b58de78",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b58de78",
        "outputId": "bcfa4b76-062b-401e-f9f1-acb6dedd1240"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (max_depth=3): 0.9666666666666667\n",
            "Accuracy (fully grown): 0.9333333333333333\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Shallow tree (max_depth=3)\n",
        "tree_shallow = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "tree_shallow.fit(X_train, y_train)\n",
        "y_pred_shallow = tree_shallow.predict(X_test)\n",
        "acc_shallow = accuracy_score(y_test, y_pred_shallow)\n",
        "\n",
        "# Fully grown tree\n",
        "tree_full = DecisionTreeClassifier(random_state=42)\n",
        "tree_full.fit(X_train, y_train)\n",
        "y_pred_full = tree_full.predict(X_test)\n",
        "acc_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "print('Accuracy (max_depth=3):', acc_shallow)\n",
        "print('Accuracy (fully grown):', acc_full)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cd4d5a9",
      "metadata": {
        "id": "6cd4d5a9"
      },
      "source": [
        "## Question 8: Train a Decision Tree Regressor on the Boston Housing dataset and print the MSE and feature importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3042940a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3042940a",
        "outputId": "cf7092c4-8d01-4975-be42-7f5eeb360e2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.495235205629094\n",
            "MedInc: 0.5285\n",
            "HouseAge: 0.0519\n",
            "AveRooms: 0.0530\n",
            "AveBedrms: 0.0287\n",
            "Population: 0.0305\n",
            "AveOccup: 0.1308\n",
            "Latitude: 0.0937\n",
            "Longitude: 0.0829\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "# Load dataset (Boston deprecated)\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "reg = DecisionTreeRegressor(random_state=42)\n",
        "reg.fit(X_train, y_train)\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "# Results\n",
        "print('Mean Squared Error:', mean_squared_error(y_test, y_pred))\n",
        "for name, importance in zip(data.feature_names, reg.feature_importances_):\n",
        "    print(f'{name}: {importance:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f3e905f",
      "metadata": {
        "id": "5f3e905f"
      },
      "source": [
        "## Question 9: Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "788481ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "788481ac",
        "outputId": "79676921-3e43-4e5f-c5cd-85f3a80aa933"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'max_depth': None, 'min_samples_split': 10}\n",
            "Best cross-validation MSE: 0.4624738299655961\n",
            "Test MSE: 0.44398484516177295\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeRegressor # Import Regressor\n",
        "from sklearn.metrics import mean_squared_error # For evaluation, though GridSearchCV uses 'scoring'\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [None, 2, 3, 4, 5],\n",
        "    'min_samples_split': [2, 3, 4, 5, 10]\n",
        "}\n",
        "\n",
        "# Use DecisionTreeRegressor and appropriate scoring for regression\n",
        "grid = GridSearchCV(DecisionTreeRegressor(random_state=42), param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1) # Use neg_mean_squared_error for regression\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print('Best parameters:', grid.best_params_)\n",
        "print('Best cross-validation MSE:', -grid.best_score_) # Convert back to positive MSE\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "print('Test MSE:', mean_squared_error(y_test, best_model.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70f94b7a",
      "metadata": {
        "id": "70f94b7a"
      },
      "source": [
        "## Question 10: Explain the step-by-step process to build a Decision Tree model for disease prediction in healthcare"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8045ff5a",
      "metadata": {
        "id": "8045ff5a"
      },
      "source": [
        "**Answer:**  \n",
        "**Step 1: Handle missing values**  \n",
        "- Impute missing numerical values with mean/median.  \n",
        "- Impute categorical values with mode or create a new category (\"Unknown\").  \n",
        "\n",
        "**Step 2: Encode categorical features**  \n",
        "- Use one-hot encoding for nominal variables.  \n",
        "- Use label encoding or ordinal mapping for ordered features.  \n",
        "\n",
        "**Step 3: Train a Decision Tree model**  \n",
        "- Start with a simple tree using `criterion='gini'` or `'entropy'`.  \n",
        "- Use `class_weight='balanced'` if data is imbalanced.  \n",
        "\n",
        "**Step 4: Tune hyperparameters**  \n",
        "- Use GridSearchCV to optimize `max_depth`, `min_samples_split`, `min_samples_leaf`, and `ccp_alpha`.  \n",
        "\n",
        "**Step 5: Evaluate performance**  \n",
        "- Use metrics like accuracy, precision, recall, F1-score, and ROC-AUC.  \n",
        "- Check confusion matrix to analyze false positives/negatives.  \n",
        "\n",
        "**Business value:**  \n",
        "- Early disease prediction helps prioritize high-risk patients.  \n",
        "- Supports doctors with data-driven decisions.  \n",
        "- Reduces healthcare costs through preventive care."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}